{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of this workflow is built apon the work done by Stephan Rasp for his CBRAIN Github repository.  However, some modifications have been made to account for changes in data, model design and different post-processing tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bash script to preprocess the data is found under /Bash_Scripts/Preprocessor.sh - an example for a 8 years training data/1 year validation data/1 year test data (selectively sampled every 10 days) is shown.  Corresponding files to help in the preprocessing are\n",
    "\n",
    "- preprocess_RG.py\n",
    "- SPCAM5.yml\n",
    "- shuffle_ds.py\n",
    "\n",
    "Once the bash script has been run, a preprocessed and shuffled training and validation dataset will be ready for use by the deep neural network.  The input and outrput variables of interest can be specified in SPCAM5.yml file. The files are also included for SPCAM3 trained neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file Deep_Training.py shows how we used manual tuning as a comparison to the SHERPA automated hyperparameter tuning packaged we used to get our best DNN. Key corresponding files include\n",
    "\n",
    "- losses.py\n",
    "- utils.py\n",
    "- models.py\n",
    "- imports.py\n",
    "- hyai_hybi.pkl\n",
    "- data_generator.py\n",
    "\n",
    "The file produces a .h5 model.\n",
    "\n",
    "We also include the scripts for the various constrained neural networks we try.  All under the Training_Scripts directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting DNN Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script Model_Predictions.py takes the .h5 model and the corresponding validation dataset to get out model predictions in a .nc file.  This file can now be used for post-processing analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Scripts for post processing for the creation of Figures, Animations and Tables can be found in the Post_Processing/ directory. The files paths are hardcoded into the jupyter notebooks and should be changed depending on the type of test data (SPCAM3 or SPCAM5) and the choice of Model predictions (Sherpa, Manual, Linear, Constrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
